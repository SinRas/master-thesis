% ---------------------------------------
% Reference Parameters
% Authors: Stephen Boyd, Lieven Vandenberghe
% Title: Convex Optimization
% Publisher: Cambride University Press
% Date: 2004

% ---------------------------------------
% Online Optimization
\subsubsection{
بهینه‌سازی محدب
\cite{convexoptimization}
}


% ---------------------------------------
% TODO
\begin{itemize}
\item
۱. مقدمه
\item
۲. مجموعه‌های محدب (کلّیت)
\item
۳. تابع‌های محدب (کلّیت)
\item
۴. مسئله‌های بهینه‌سازی محدب (کلّیت)
\item
۶. تخمین و فیت کردن (بخون)
\item
۷. تخمین آماری (بخون)
\item
۹. بهینه‌سازی بدون قید (الگوریتم‌ها و نرخ‌های همگرایی)
\item
۱۱. روش نقطه‌ی درونی (بخون)

\end{itemize}


% ---------------------------------------
% Mathemtical Optimization
\subsubsubsection{
بهینه‌سازی ریاضی
\LTRfootnote{
Mathematical Optimization
}
}
یک مسئله‌ی بهینه‌سازی ریاضی یا به عبارت ساده‌تر 
\emph{
مسئله‌ی بهینه‌سازی
\LTRfootnote{
Optimization Problem
}
} 
به شکل زیر است:
\[
\begin{array}{lll}
\mbox{minimize} & \mthfnc{f}_0(x) & \\
\mbox{subject to} & \mthfnc{f}_i(x) \leq b_i, & i = 1, \cdots, m
\end{array}
\]
که در آن 
$x = (x_1, \cdots, x_n)$ 
متغیرهای بهینه‌سازی مسئله، تابع 
$\mthfnc{f}_0: \hollow{R}^n \rightarrow \hollow{R}$ 
تابع هدف
\LTRfootnote{
Objective Function
} 
و تابع‌های 
$\mthfnc{f}_i: \hollow{R}^n \rightarrow \hollow{R}$ 
تابع‌های قید
\LTRfootnote{
Constraint Functions
} 
هستند. بردار 
$x^*$ 
را بهینه
\LTRfootnote{
Optimal
} 
یا پاسخ مسئله گویند اگر در میان بردارهایی که در قید‌ها صدق می‌کنند، کمترین مقدار تابع هدف را داشته باشد.
\[
\forall z \in \hollow{R}^n: \mthfnc{f}_1(z) \leq b_1 , \cdots, \mthfnc{f}_m(z) \leq b_m \Rightarrow \mthfnc{f}_0(z) \geq \mthfnc{f}_0(x^*)
\]

به یک مسئله‌ی بهینه‌سازی، برنامه‌ریزی خطی
\LTRfootnote{
Linear Program
} 
یا بهینه‌سازی خطی
\LTRfootnote{
Linear Optimization
} 
گویند هرگاه (تابع هدف و تابع‌های قید همگی خطی باشند):
\[
\forall \alpha,\beta \in \hollow{R} \; : \; \mthfnc{f}_i( \alpha x + \beta y ) = \alpha \mthfnc{f}_i(x) + \beta \mthfnc{f}_i(y) \hspace{5mm} i = 0,1,\cdots,m
\]

هرگاه تابع هدف و تابع‌های قید همگی در ویژگی زیر صدق کنند (محدب
\LTRfootnote{
Convex
} 
باشند) مسئله را بهینه‌سازی محدب
\LTRfootnote{
Convext Optimization
} 
می‌نامند:
\[
\forall \alpha,\beta \in [0,1] \; , \; \alpha + \beta = 1 \; : \; \mthfnc{f}_i( \alpha x + \beta y ) \leq \alpha \mthfnc{f}_i(x) + \beta \mthfnc{f}_i(y) \hspace{5mm} i = 0,1,\cdots,m
\]


% ---------------------------------------
% Applications
\subsubsubsection{
کاربردها
\LTRfootnote{
Applications
}
}

مسئله‌ی بهینه‌سازی ***، مسئله‌ی انتخاب 
\textbf{
بهترین
} 
بردار 
$x^* \in \hollow{R}^n$ 
از میان بردارهایی است که در شرایط 
$\mthfnc{f}_i(x) \leq b_i$ 
صدق می‌کنند. در جمله‌ی پیش، 
\textbf{
بهترین
} 
کیفیتی است که به وسیله‌ی تابع 
$\mthfnc{f}_0: \hollow{R}^n \rightarrow \hollow{R}$ 
کمّی شده است به این معنی که به با ثابت نگاه داشتن تابع‌های قید (نقاطی که در میان آن‌ها به دنبال پاسخ هستیم)، می‌توان کمّی سازی‌های متفاوت و احتمالا نتایج متفاوتی برای 
\emph{
بهترین
} 
بردار بدست آورد.

چند مثال از مسئله‌هایی که در چهارچوب مسائل بهینه‌سازی قرار می‌گیرند.

% Portfolio Optimization
\subsubsubsection{
بهینه‌سازی پورتفولیو
\LTRfootnote{
Portfolio Optimization
}:
} 
در مسئله‌ی بهینه‌سازی پورتفولیو، ما به دنبال سرمایه‌گذاری سرمایه‌ی
\LTRfootnote{
Capital
} 
خود در چند دارایی
\LTRfootnote{
Asset
} 
هستیم. متغیر 
$x_i$ 
نشان دهنده‌ی سرمایه‌گذاری در دارایی 
$i$
ام است و بردار 
$x \in \hollow{R}^n$ 
نشان دهنده‌ی اختصاص پورتفولیو به دارایی‌های مختلف است. قیدها به طور مثال می‌توانند محدودیت‌هایی بر سرمایه، منفی نبودن سرمایه‌ی اختصاص داده شده و حداقل سود بردار سرمایه‌گذاری باشند. هدف می‌تواند کم کردن مقدار ریسک
\LTRfootnote{
Risk
} 
پورتفولیو باشد.


% Stock
\subsubsubsection{
پیش‌بینی سهام (آفلاین)
\LTRfootnote{
(Offline) Stock Prediction
}
} 
تابع‌های 
$\mthfnc{e}_i : \hollow{R}^d \rightarrow \hollow{R} \; i=1,\cdots,k$، 
و جفت نقطه‌های 
$\{ (x_j, y_j) \}^{N}_{j=1}$ 
داده شده‌اند. هدف یافتن ضریب‌های 
$\alpha = (\alpha_i) \in \hollow{R}^k \; \sum^{k}_{i=1} \alpha_i = 1$ 
است به گونه‌ای که تابع هزینه‌ی داده شده 
$\mthfnc{l} : \hollow{R} \rightarrow \hollow{R}$ 
کمینه شود. ابتدا تابع پیش‌بینی
\[
\mthfnc{f}(\alpha, x) = \sum^{k}_{i=1} \alpha_i \mthfnc{e}_i(x)
\]
و همچنین تابع هدف تجمعی 
\[
\mthfnc{L}(\alpha) = \sum^{N}_{j=1} \mthfnc{l} ( \mthfnc{f}(\alpha, x_j) , y_j )
\]
حال مسئله‌ی بهینه‌سازی را به صورتی که از پیش دیده‌ایم باز نویسی می‌کنیم:
\[
\begin{array}{lll}
\mbox{minimize} & \mthfnc{L}(\alpha) & \\
\mbox{subject to} & \alpha_i \geq 0 & i = 1,\cdots, k\\
& \sum^{k}_{i=1} \alpha_i = 1 & 
\end{array}
\]

در واقع در این مسئله، اطلاعات 
$k$ 
متخصص
\LTRfootnote{
Expert
} 
داده شده است و هدف ما این است که با توجه به سابقه‌ای که از عملکرد در داده‌های 
$\{ (x_j, y_j) \}^{N}_{j=1}$ 
دیده می‌شود، تصمیم بگیریم که چقدر به هریک از متخصص‌ها اعتماد کنیم (به پیش‌بینی او، وزن نسبت دهیم).

در نسخه‌ی ارائه شده از این مسئله، تمامی داده‌ها همزمان دانسته فرض شده‌اند و ما به دنبال کمینه کردن خطای پیش‌بینی در آن‌ها هستیم. این مسئله هم بدون در نظر گرفتن منظم‌سازی
\LTRfootnote{
Regularization
} 
صورت‌بندی شده است و هم بدون در نظر گرفتن دریافت برخط
\LTRfootnote{
Online
} 
داده‌ها. در بخش‌های آینده به این دو ویژگی پرداخته خواهد شد.

% Traffic
\subsubsubsection{
تخمین مسیر / زمان سفر (آفلاین)
\LTRfootnote{
Route Finding \ ETA Estimation
}
}
نقشه‌ی یک شهر را به شکل یک گراف 
$G = (\set{V},\set{E})$ 
در نظر بگیرید که در آن 
$\set{V}$ 
مجموعه‌ی تقاطع‌ها و نقاطع تولید سفر، و 
$\set{E}$ 
مجموعه‌ی خیابان‌ها و مسیرها است. یک جفت نقطه‌ی متمایز را بر روی نقشه انتخاب کنید، 
$(src, dst) \in \set{V} \times \set{V}$ 
مجموعه‌ی مسیرهای ساده
\LTRfootnote{
Simple Path
} 
(بدون دور) شروع شده از 
$src$ 
و تمام شده در 
$dst$ 
را در نظر بگیرید:
\[
\set{P}(src,dst) = \{ \mbox{all paths from } src \mbox{ to } dst \} = \{ p_i \}^{k}_{i=1}
\]
می‌خواهیم با داشتن زمان سفر متوسط هر یک از یال‌ها (که همان تابع توزیع ترافیک است) در چند روز مختلف،
\[
\mthfnc{t}_j : \set{E} \rightarrow \hollow{R}^+ \; j = 1,\cdots,N
\] 
زمان و مسیر کوتاه ترین سفر میان مبداء و مقصد تعیین شده را تخمین بزنیم. در این مسئله به دنبال توزیع وزن میان مسیرهای مختلف بین مبداء و مقصد هستیم. یعنی به دنبال نقطه‌ی 
$\alpha \in [0,1]^k$ 
به گونه‌ای که 
$\sum^{k}_{i=1} \alpha_i = 1$. 
تابع خطای جزئی و تجمعی نیز به شکل زیر تعریف می‌شوند:
\[
\begin{split}
&\mthfnc{l}(\alpha, \mthfnc{t}_j) = \sum^{k}_{i=1} \alpha_i \sum_{e \in p_i} \mthfnc{t}_j(e)\\
&\mthfnc{L}(\alpha) = \sum^{N}_{j=1} \mthfnc{l}( \alpha, \mthfnc{t}_j )
\end{split}
\]
صورت بندی به شکل مسئله‌ی بهینه‌سازی در زیر آمده است:
\[
\begin{array}{lll}
\mbox{minimize} & \mthfnc{L}(\alpha) & \\
\mbox{subject to} & \alpha_i \geq 0 & i = 1,\cdots, k\\
& \sum^{k}_{i=1} \alpha_i = 1 & 
\end{array}
\]

در اینجا نیز مسئله به این صورت قابل توصیف است که میان دو نقطه‌ی مبداء و مقصد 
$(src, dst)$ 
تعداد 
$k$ 
مسیر داده شده است. هدف تخصیص وزن به مسیرهای مختلف است به گونه‌ای که متوسط زمان سفر در مشاهده‌های ترافیک گراف 
$\{ \mthfnc{t}_j \}^{N}_{j=1}$ 
در مسیرهای وزن‌دار، کمینه شود. زیاد بودن وزن مسیر 
$p_i$ 
نسبت به مسیر 
$p_{i^\prime}$، 
نمایانگر کوتاه‌تر بودن زمان سفر در تاریخچه‌ی مشاهده شده در مسیر 
$p_i$ 
نسبت به 
$p_{i^\prime}$ 
با توجه به تابع هزینه‌ی 
$\mthfnc{l}$ 
است.





% ---------------------------------------
% Solving Optimization Problems
\subsubsubsection{
حل کردن مسئله‌های بهینه‌سازی
}
یک روش حل
\LTRfootnote{
Solution Method
} 
برای دسته‌ای از مسئله‌های بهینه‌سازی، یک الگوریتم
\LTRfootnote{
Algorithm
} است که جواب بهینه را با داده شدن یک نمونه از مسئله‌
\LTRfootnote{
Instance of Problem
} 
محاسبه می‌کند. از اواخر دهه‌ی ۱۹۴۰، تلاش‌های زیادی در راستای توسعه‌ی الگوریتم‌هایی برای حل دسته‌های مختلفی از مسئله‌های بهینه‌سازی، تحلیل ویژگی‌های آن‌ها و توسعه‌ی پیاده‌سازی‌های خوب نرم‌افزاری از آنها، شده است. موثر بودن این الگوریتم‌ها، به طور مثال توانایی آن‌ها در حل مسئله‌ی بهینه‌سازی ***، تفاوت‌های مشهود و وابسته به عواملی همچون شکل تابع‌های هدف و قید، تعداد متغیرها و قیدها، و ساختارهای خاص (به طور مثال تنکی
\LTRfootnote{
Sparsity
}) 
دارد.

%%%%%%%%%%%

حتی زمانی که تابع هدف و تابع‌های قید هموار
\LTRfootnote{
Smooth
} 
باشند (به طور مثال چند جمله‌ای‌ها)، حل مسئله‌ی بهینه‌سازی *** به طرز شگفت‌انگیزی دشوار است. به همین دلیل رویکردهای بررسی مسئله‌ی کلی، دارای برخی سازش‌ها
\LTRfootnote{
Compromise
} 
همچون زمان بسیار طولانی محاسبه و یا احتمال پیدا نکردن جواب، هستند.

%%%%%%%%%%%

اما در این میان، برخی استثناء‌ها نیز وجود دارند. برای برخی دسته از مسئله‌ها، الگوریتم‌های موثری وجود دارد که جواب را با اطمینان می‌یابند (حتی برای مسئله‌هایی با صدها یا هزارها متغیر و قید). از جمله‌ی این کلاس‌ها، می‌توان به برنامه‌ریزی خطی
\LTRfootnote{
Linear Programming
}، 
مسئله‌های کمینه‌ی مربعات
\LTRfootnote{
Least-Squares Problems
} 
و بهینه‌سازی محدب اشاره کرد.


% ---------------------------------------
% Nonlinear Optimization
\subsubsubsection{
بهینه‌سازی غیرخطی
}


































