% ---------------------------------------
% Reference Parameters
% Author: Sebastien Bubeck
% Title: Introduction to Online Optimization
% Publisher: Princeton University - Department of Operations Research and Financial Engineering
% Date: December 14, 2011

% ---------------------------------------
% TODO
% 
%
%
%
%
%
%
%
%
%
%
%

% ---------------------------------------
% Online Optimization
\subsubsection{
بهینه‌سازی برخط
\cite{onlinelearningbubeck}
}

% ---------------------------------------
% Statistical Learning Theory
\subsubsubsection{
نظریه‌ی یادگیری آماری
\LTRfootnote{
Statistical Learning Theory
}
}

در دنیایی که جمع‌آوری خودکار داده فراگیر شده است، آماردان‌ها باید چهارچوب‌های فکری خودشان را با مسئله‌های جدید مطابقت دهند. هر زمان که صحبت از شبکه‌ی اینترنت، داده‌های مشتری
\LTRfootnote{
Consumer Data
}
، و یا بازار مالی
\LTRfootnote{
Financial Market
}
 با میان می‌آید، یک ویژگی مشترک به چشم می‌آید: حجم بسیار زیاد داده که نیازمند فهم و تحلیل سریع است. شرایط مسئله‌های کنونی با آمار کلاسیک بسیار تفاوت دارد، تعداد مشاهده‌ها بسیار زیاد و تعداد متغیرهای مرتبط بسیار کم است. یکی از مدل‌های به تفصیل بررسی شده برای یادگیری، چهارچوب نظریه‌ی یادگیری آماری است. که به اختصار در ادامه بررسی خواهد شد:

% Definition
\begin{quote}
{\bf
پروتکل
\LTRfootnote{
Protocol
}
 یادگیری آماری:
} 
پروتکل ابتدایی یادگیری آماری به شکل زیر است:
\begin{itemize}
\item
مشاهد کنید نمونه‌های 
$Z_1, \cdots, Z_n \in \randomvariable{Z}$. 
فرض می‌کنیم که توزیع مشاهده‌ها مستقل و هم‌توزیع
\LTRfootnote{
I.I.D.
}
 از توزیع احتمال 
$\hollow{P}$
آمده اند.

\item
تصمیم بگیرید (یا عملی را انتخاب کنید) 
$a( Z_1, \cdots, Z_n ) \in \set{A}$ 
که 
$\set{A}$ 
یک مجموعه‌ی داده شده از تصمیم‌های ممکن است.

\item
به اندازه‌ی متوسط ضرر
\LTRfootnote{
Loss
} 
$\hollow{E}_{Z \sim \hollow{P}} \mthfnc{l}( a(Z_1,\cdots,Z_n), Z )$ 
هزینه بپردازید، که در آن 
$\mthfnc{l} : \set{A} \times \randomvariable{Z} \rightarrow \hollow{R}_+$ 
تابع ضرر داده شده است.
\end{itemize}

{\bf
هدف:
} 
کمینه کردن (و کنترل) ریسک مازاد
\LTRfootnote{
Excess Risk
}
:
\[
r_n = \hollow{E}_{Z \sim \hollow{P}} \mthfnc{l}( a(Z_1, \cdots, Z_n), Z ) - \inf_{a \in \set{A}} \hollow{E}_{Z \sim \hollow{P}} \mthfnc{l} ( a, Z )
\]
که معیار اندازه‌گیری میزان متوسط ضرر متحمل شده در مقایسه با انتخاب بهینه است.

\end{quote}



% Remark
\begin{quote}
{\bf
تعریف:
} 
کنترل کردن ریسک مازاد، به معنی پیدا کردن کران بالا برای 
$r_n$ 
است که یا در امید ریاضی
\LTRfootnote{
Expectation
} 
صدق کند ( نسبت به دنباله‌ی 
$Z_1,\cdots,Z_n$ 
) و یا با احتمال حداقل 
$1 - \delta$. 
معمولا کران بالا به شکلی عبارتی است بر اساس معیارهایی از پیچیدگی
\LTRfootnote{
Complexity Measure
} 
$\set{A}$ 
و 
$\mthfnc{l}$. 
همچنین اگر کران بالا به توزیع 
$\hollow{P}$ 
بستگی داشته باشد، می‌گوییم کران بالا وابسته به توزیع
\LTRfootnote{
Distribution-Dependent
} 
است و در غیر این صورت گوییم که آزاد از توزیع
\LTRfootnote{
Distribution-Free
} 
است.

\end{quote}


فرمول‌بندی بالا کلّی بوده و باعث می‌شود که مسئله‌های بسیاری را در بر گیرد. در ادامه به چند مثال اشاره می‌کنیم:

% Example
{\bf
مثال: تخمین رگرسیون
\LTRfootnote{
Regression Estimation
}
} 
\begin{itemize}
\item
داده‌ی مشاهده شده جفت‌هایی از نقطه‌ها به شکل 
$ Z_i = ( X_i, Y_i ) \in \set{X} \times \set{Y} $ 
هستند.


\item
مجموعه‌ی ممکن انتخاب‌ها، مجموعه‌ی تابع‌های از 
$\set{X}$ 
به 
$\set{Y}$ 
است، 
$\set{A} \subset \{ \mthfnc{f} : \set{X} \rightarrow \set{Y} \} $.


\item
تابع هزینه 
$\mthfnc{l}(a, (x,y) )$ 
میزان دقت پیش‌بینی تابع 
$\mthfnc{a}: \set{X} \rightarrow \set{Y}$ 
را اندازه‌گیری می‌کند. به عنوان نمونه اگر 
$\set{Y}$ 
یک فضای نرم‌دار
\LTRfootnote{
Normed Space
}
 باشد، یک انتخاب معمول 
$\mthfnc{l}(a, (x,y)) = || a(x) - y ||$ 
خواهد بود.

\end{itemize}

% Example
{\bf
مثال: تکمیل ماتریس
\LTRfootnote{
Matrix Completion
}
 (یا فیلتر کردن مبتنی بر همکاری
\LTRfootnote{
Collaborative Filtering
}
 )
} 
تکمیل ماتریس یک مثال از یادگیری در ابعاد بالا
\LTRfootnote{
High-Dimensional Learning
} 
است. در این مسئله 
$\hollow{P}$ 
یک توزیع یکنواخت بر روی درایه‌های مجهول ماتریس 
$ M \in \hollow{R}^{m \times d }$، 
و هدف بازسازی ماتریس 
$M$ 
است. سازوکار ابعاد بالا مربوط می‌شود به شرایط 
$ n \ll m \times d$ 
برقرار باشد. برای عملی
\LTRfootnote{
Feasible
} 
کردن مسئله، یک فرض طبیعی این است که ماتریس 
$M$ 
رتبه‌ی
\LTRfootnote{
Rank
} 
کمی دارد در مقایسه با تعداد نمونه‌ها 
$n$، 
بنابراین خواهیم داشت 
$k < n \ll m \times d$. 
تابع ضررهای مختلفی می‌توان در نظر گرفت، چه در مسئله‌ی پیش‌بینی و چه در مسئله‌ی تخمین در کامل‌سازی ماتریس.




% ---------------------------------------
% Statistical Learning Theory
\subsubsubsection{
یادگیری برخط
\LTRfootnote{
Online Learning
}
}

با وجود موفقیت‌های بسیاری که نظریه‌ی یادگیری آماری بدست آورده، در زمینه‌ی پویایی اطلاعات با حجم بالا
\LTRfootnote{
Dynamic Aspect of Massive Data
} 
ناموفق عمل کرده است. یادگیری برخط، تلاشی است که فائق آمدن بر این مشکل. در ادامه‌ی این نوشته ما موضوع را بهینه‌سازی برخط نام خواهیم نهاد زیرا به پروتکل ارائه شده نزدیک‌تر است.



\begin{quote}
{\bf
پروتکل بهینه‌سازی برخط
}

یادگیری برخط گسترش طبیعی یادگیری آماری است. به تعبیری می‌توان یادگیری برخط را به عنوان روشی در مرزهای نتایج آزاد از توزیع، دید. در واقع تفاوت بنیادی میان یادگیری برخط و یادگیری آماری، به علاوه‌ی اینکه داده به شکل دنباله‌ای وجود دارد، این حقیقت است که فرض احتمالاتی‌ای در رابطه با دنباله‌ی داده 
$Z_1, \cdots, Z_n$ 
انجام نمی‌شود. با کاهش مسئله به ساختار مینیمال
\LTRfootnote{
Minimal
} 
آن، می‌توان امیدوار بود که به سختی بنیادی در مسئله‌های یادگیری رسید. همانگونه که مشخص شده است، این تغییر نگرش به مسئله، بسیار مثمر ثمر است و تاثیری عمیق و بنیادین بر چشم‌انداز نوین یادگیری ماشین گذاشته است.


پروتکل یادگیری برخط به شکل رسمی در ادامه آمده است، در هر گام از زمان 
$t = 1, 2, \cdots, n$: 

\begin{itemize}
\item
عمل 
$a_t \in \set{A}$ 
را انتخاب کنید

\item
همزمان یک فرد متخاصم
\LTRfootnote{
Adversary
} 
(یا طبیعت) انتخاب می‌کند
$z_t \in \randomvariable{Z}$

\item
ضرر شرایط کنونی را متحمل می‌شوید 
$\mthfnc{l}(a_t, z_t)$

\item
عمل فرد متخاصم (یا طبیعت) را مشاهده می‌کنید 
$z_t$

\end{itemize}

هدف کمینه کردن (و کنترل) پشیمانی
\LTRfootnote{
Regret
} 
تجمعی است:
\[
R_n = \sum^{n}_{i = 1} \mthfnc{l}(a_t,z_t) - \inf_{a \in \set{A}} \sum^{n}_{t=1} \mthfnc{l}(a,z_t)
\]

\end{quote}

در کلام، پشیمانی تجمعی مقدار ضرر تجمعی بازیکن را با ضرر تجمعی بهترین عمل (با اطلاعات موجود) مقایسه می‌کند. در اینجا تلاش برای یافتن کران‌هایی ( به طور مثلا کران‌های بالا برای 
$R_n$) 
مستقل از انتخاب‌های فرد متخاصم هستیم.

% Example
{\bf
مثال: رگرسیون برخط، دسته‌بندی
\LTRfootnote{
Classification
} 
برخط
} 

در هر گام، بازیکن یک تابع رگرسیون انتخاب می‌کند 
$\mthfnc{a}_t : \set{X} \rightarrow \set{Y}$ 
و متخاصم نیز یک جفت ورودی/خروجی 
$(x_t, y_t) \in \set{X} \times \set{Y}$ 
انتخاب می‌کند. بازیکن ضرر 
$\mthfnc{l}( \mthfnc{a}_t, (x_t, y_t) )$ 
را متحمل می‌شود و جفت 
$(x_t, y_t)$ 
را مشاهده می‌کند. به طور معمول مجموعه‌ی 
$\set{A}$ 
به خانواده‌ی کوچکی از تابع‌های رگرسیون محدود می‌شود، مانند ابر صفحه‌های تصمیم
\LTRfootnote{
Decision Hyperplane
} 
در الگویابی
\LTRfootnote{
Pattern Recognition
} 
برخط.



% Example
{\bf
مثال: سرمایه‌گذاری پی‌درپی
\LTRfootnote{
Sequential Investment
}
} 

یک بازار سهام ایده‌آل
\LTRfootnote{
Idealized Stock Market
} 
با 
$d$ 
دارایی
\LTRfootnote{
Asset
} 
را در نظر بگیرید. این بازار را با بردار 
$z \in \hollow{R}^d_+$ 
نمایش دهید که نشان دهنده‌ی قیمت‌های نسبی در یک بازه‌ی زمانی است. به این معنی که اگر سرمایه‌گذاری 
$a \in \hollow{R}^d_+$ 
انجام شود ( 
$a(i)$ 
مقدار در دارایی 
$i$ 
سرمایه‌گذاری شده)، برگشت سرمایه به اندازه‌ی 
$ \sum^{d}_{i=1} a(i) z(i) = a^T z$ 
خواهد بود.

حال به بررسی مسئله‌ی سرمایه‌گذاری پی‌درپی در این بازار سهام می‌پردازیم. در هر بازه‌ی معامله 
$t$، 
سرمایه
\LTRfootnote{
Capital
} 
کنونی را با 
$W_{t-1}$ 
نمایش دهید و بازیکن تمام سرمایه‌ی کنونی را با توجه به نسبت‌های 
$a_t \in \set{A} = \left\lbrace a \in \hollow{R}^d_+ , \sum^{d}_{i=1} a(i) = 1 \right\rbrace$ 
سرمایه گذاری می‌کند. همزمان بازار بردار 
$z_t \in \hollow{R}^d_+$ 
را مشخص می‌کند. ثروت در پایان دوره‌ی 
$t$ 
به شکل زیر است:
\[
W_t = \sum^{d}_{i=1} a_t (i) W_{t-1} z_t (i) = W_{t-1} a^T_t z_t = W_0 \prod^{t}_{s=1} a^T_s z_s
\]

یک خانواده‌ی مهم از استراتژی‌های سرمایه‌گذاری در این مسئله، مجموعه‌ی پورتفولیوهای متعادل شونده ثابت
\LTRfootnote{
Constantly Rebalanced Portfolio
} 
هستن، که به مجموعه‌ی تصمیم‌هایی به شکل 
$a_t = a, \forall t \geq 1$ 
تعلق دارند. به بیان دیگر، بازی‌کن در هر بازه‌ی معامله‌ی 
$t$ 
سرمایه‌ی خود 
$W_{t-1}$ 
را با توجه به نسبت‌های 
$a$ 
تقسیم می‌کند.

هدف مسئله، قابل رقابت بودن با این خانواده از استراتژی‌ها است که می‌توان آن را با ثروت نسبی رقابتی
\LTRfootnote{
Competetive Wealth Ratio
} 
سنجید:
\[
\sup_{a \in \set{A}} \left\lbrace \frac{ W^a_n }{ W_n } = \frac{ W_0 \prod^{n}_{s=1} a^T z_s }{ W_n } \right\rbrace
\]

بنابراین برای ثروت نسبی لگاریتمی
\LTRfootnote{
Logarithmic Wealth Ratio
} 
خواهیم داشت:
\[
\sum^{n}_{t=1} - \log ( a^T_t z_t ) - \inf_{a \in \set{A}} \sum^{n}_{t=1} - \log (a^T z_t)
\]
که در واقع پشیمانی تجمعی برای مسئله‌ی بهینه‌سازی برخط بر روی سادک
\LTRfootnote{
Simplex
} 
$d-1$ 
بعدی با تابع هزینه‌ی لگاریتمی 
$\mthfnc{l} (a,z) = -\log (a^T z)$ 
است.












































